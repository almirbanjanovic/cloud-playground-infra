# ============================================================================
# KAITO Custom Model - Option 4: Azure ML Model Registry
# ============================================================================
# Use this manifest when your model is registered in Azure Machine Learning
# and you want to download it at pod startup.
#
# Prerequisites:
#   1. Register your model in Azure ML workspace
#   2. Enable workload identity on your AKS cluster
#   3. Create a managed identity with "AzureML Data Scientist" role on the ML workspace
#   4. Create a federated credential linking the managed identity to a K8s service account
#   5. Create the service account with the workload identity annotation
#
# Setup Commands:
#   # Create service account with workload identity
#   kubectl create serviceaccount kaito-model-loader -n <namespace>
#   kubectl annotate serviceaccount kaito-model-loader -n <namespace> \
#     azure.workload.identity/client-id="<managed-identity-client-id>"
#
# Docs:
#   - Azure ML SDK v2: https://learn.microsoft.com/en-us/python/api/overview/azure/ai-ml-readme
#   - ModelOperations.download(): https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.operations.modeloperations
#   - DefaultAzureCredential: https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential
#   - AKS Workload Identity: https://learn.microsoft.com/en-us/azure/aks/workload-identity-overview
#   - HuggingFace Accelerate: https://huggingface.co/docs/accelerate
# ============================================================================

apiVersion: kaito.sh/v1beta1
kind: Workspace
metadata:
  name: ${name}
  namespace: ${namespace}
  # annotations:
  #   kaito.sh/enablelb: "True"  # Creates LoadBalancer service automatically (testing only, not for production)

resource:
  instanceType: ${instanceType}
  labelSelector:
    matchLabels:
      apps: ${appLabel}

inference:
  template:
    metadata:
      labels:
        azure.workload.identity/use: "true"    # Required for workload identity
    spec:
      serviceAccountName: kaito-model-loader   # Service account with workload identity

      initContainers:
        - name: download-azureml-model
          image: python:3.10-slim
          command: ["sh", "-c"]
          args:
            - |
              pip install --quiet azure-ai-ml azure-identity
              python << 'EOF'
              from azure.ai.ml import MLClient
              from azure.identity import DefaultAzureCredential

              credential = DefaultAzureCredential()
              ml_client = MLClient(
                  credential=credential,
                  subscription_id="<subscription-id>",
                  resource_group_name="<resource-group>",
                  workspace_name="<workspace-name>"
              )

              # Download a registered model
              # This downloads to: /models/<model-name>/<version>/
              ml_client.models.download(
                  name="<model-name>",
                  version="<version>",
                  download_path="/models"
              )
              print("Model downloaded successfully")
              EOF
          volumeMounts:
            - name: model-volume
              mountPath: /models
          # Note: DefaultAzureCredential automatically uses workload identity
          # when AZURE_CLIENT_ID, AZURE_TENANT_ID, and AZURE_FEDERATED_TOKEN_FILE
          # are injected by AKS.

      containers:
        - name: custom-llm-container
          image: mcr.microsoft.com/aks/kaito/kaito-base:0.0.8

          resources:
            requests:
              memory: "8Gi"
              cpu: "2"
            limits:
              memory: "16Gi"
              cpu: "4"

          env:
            # Please refer to the documentation for more information.
            # https://huggingface.co/docs/transformers/main/en/main_classes/text_generation
            #
            # HuggingFace tokenizers parallelism - disable to prevent potential deadlocks
            - name: TOKENIZERS_PARALLELISM
              value: "false"

          livenessProbe:
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 300
            periodSeconds: 30
            timeoutSeconds: 5
            failureThreshold: 3

          readinessProbe:
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3

          command:
            - "accelerate"
          args:
            - "launch"
            - "--num_processes"
            - "1"
            - "--num_machines"
            - "1"
            - "--gpu_ids"
            - "all"
            - "tfs/inference_api.py"
            - "--pipeline"
            - "text-generation"
            - "--trust_remote_code"
            - "--allow_remote_files"
            - "--pretrained_model_name_or_path"
            - "/models/<model-name>/<version>"    # Path where Azure ML downloads model
            - "--torch_dtype"
            - "float16"                    # Use "float16" for V100 GPUs; use "bfloat16" for A100, H100 or newer

          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: model-volume
              mountPath: /models

      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
        - name: model-volume
          emptyDir: {}    # Ephemeral - model re-downloads on pod restart
